{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvEh3HYdaaGy",
        "outputId": "7441e456-6448-4cd3-c846-28633a48142b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-55b45ef5428f>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data = torch.load(data_file)  # Shape: (19071, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.4540, Training Accuracy: 37.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/100], Loss: 1.9112, Training Accuracy: 48.04%\n",
            "Epoch [3/100], Loss: 1.6649, Training Accuracy: 49.87%\n",
            "Epoch [4/100], Loss: 1.4826, Training Accuracy: 57.51%\n",
            "Epoch [5/100], Loss: 1.3357, Training Accuracy: 62.90%\n",
            "Epoch [6/100], Loss: 1.2159, Training Accuracy: 70.72%\n",
            "Epoch [7/100], Loss: 1.1299, Training Accuracy: 73.93%\n",
            "Epoch [8/100], Loss: 1.0572, Training Accuracy: 76.08%\n",
            "Epoch [9/100], Loss: 1.0008, Training Accuracy: 76.94%\n",
            "Epoch [10/100], Loss: 0.9543, Training Accuracy: 78.49%\n",
            "Epoch [11/100], Loss: 0.9174, Training Accuracy: 79.44%\n",
            "Epoch [12/100], Loss: 0.8864, Training Accuracy: 80.34%\n",
            "Epoch [13/100], Loss: 0.8619, Training Accuracy: 80.87%\n",
            "Epoch [14/100], Loss: 0.8408, Training Accuracy: 81.18%\n",
            "Epoch [15/100], Loss: 0.8243, Training Accuracy: 81.55%\n",
            "Epoch [16/100], Loss: 0.8079, Training Accuracy: 81.68%\n",
            "Epoch [17/100], Loss: 0.7938, Training Accuracy: 81.88%\n",
            "Epoch [18/100], Loss: 0.7820, Training Accuracy: 81.99%\n",
            "Epoch [19/100], Loss: 0.7719, Training Accuracy: 82.28%\n",
            "Epoch [20/100], Loss: 0.7662, Training Accuracy: 82.13%\n",
            "Epoch [21/100], Loss: 0.7522, Training Accuracy: 82.35%\n",
            "Epoch [22/100], Loss: 0.7444, Training Accuracy: 82.48%\n",
            "Epoch [23/100], Loss: 0.7376, Training Accuracy: 82.43%\n",
            "Epoch [24/100], Loss: 0.7305, Training Accuracy: 82.46%\n",
            "Epoch [25/100], Loss: 0.7233, Training Accuracy: 82.47%\n",
            "Epoch [26/100], Loss: 0.7237, Training Accuracy: 82.47%\n",
            "Epoch [27/100], Loss: 0.7131, Training Accuracy: 82.38%\n",
            "Epoch [28/100], Loss: 0.7070, Training Accuracy: 82.54%\n",
            "Epoch [29/100], Loss: 0.7031, Training Accuracy: 82.52%\n",
            "Epoch [30/100], Loss: 0.7002, Training Accuracy: 82.49%\n",
            "Epoch [31/100], Loss: 0.6963, Training Accuracy: 82.50%\n",
            "Epoch [32/100], Loss: 0.6901, Training Accuracy: 82.64%\n",
            "Epoch [33/100], Loss: 0.6870, Training Accuracy: 82.65%\n",
            "Epoch [34/100], Loss: 0.6856, Training Accuracy: 82.62%\n",
            "Epoch [35/100], Loss: 0.6806, Training Accuracy: 82.67%\n",
            "Epoch [36/100], Loss: 0.6792, Training Accuracy: 82.69%\n",
            "Epoch [37/100], Loss: 0.6772, Training Accuracy: 82.64%\n",
            "Epoch [38/100], Loss: 0.6736, Training Accuracy: 82.62%\n",
            "Epoch [39/100], Loss: 0.6730, Training Accuracy: 82.72%\n",
            "Epoch [40/100], Loss: 0.6690, Training Accuracy: 82.82%\n",
            "Epoch [41/100], Loss: 0.6653, Training Accuracy: 82.77%\n",
            "Epoch [42/100], Loss: 0.6638, Training Accuracy: 82.83%\n",
            "Epoch [43/100], Loss: 0.6614, Training Accuracy: 82.93%\n",
            "Epoch [44/100], Loss: 0.6595, Training Accuracy: 82.98%\n",
            "Epoch [45/100], Loss: 0.6586, Training Accuracy: 83.02%\n",
            "Epoch [46/100], Loss: 0.6531, Training Accuracy: 83.08%\n",
            "Epoch [47/100], Loss: 0.6537, Training Accuracy: 83.06%\n",
            "Epoch [48/100], Loss: 0.6523, Training Accuracy: 83.04%\n",
            "Epoch [49/100], Loss: 0.6503, Training Accuracy: 83.14%\n",
            "Epoch [50/100], Loss: 0.6487, Training Accuracy: 83.14%\n",
            "Epoch [51/100], Loss: 0.6499, Training Accuracy: 83.17%\n",
            "Epoch [52/100], Loss: 0.6464, Training Accuracy: 83.15%\n",
            "Epoch [53/100], Loss: 0.6444, Training Accuracy: 83.19%\n",
            "Epoch [54/100], Loss: 0.6435, Training Accuracy: 83.29%\n",
            "Epoch [55/100], Loss: 0.6416, Training Accuracy: 83.25%\n",
            "Epoch [56/100], Loss: 0.6394, Training Accuracy: 83.27%\n",
            "Epoch [57/100], Loss: 0.6415, Training Accuracy: 83.29%\n",
            "Epoch [58/100], Loss: 0.6389, Training Accuracy: 83.31%\n",
            "Epoch [59/100], Loss: 0.6391, Training Accuracy: 83.19%\n",
            "Epoch [60/100], Loss: 0.6366, Training Accuracy: 83.30%\n",
            "Epoch [61/100], Loss: 0.6346, Training Accuracy: 83.28%\n",
            "Epoch [62/100], Loss: 0.6337, Training Accuracy: 83.32%\n",
            "Epoch [63/100], Loss: 0.6351, Training Accuracy: 83.30%\n",
            "Epoch [64/100], Loss: 0.6316, Training Accuracy: 83.32%\n",
            "Epoch [65/100], Loss: 0.6310, Training Accuracy: 83.40%\n",
            "Epoch [66/100], Loss: 0.6325, Training Accuracy: 83.32%\n",
            "Epoch [67/100], Loss: 0.6309, Training Accuracy: 83.38%\n",
            "Epoch [68/100], Loss: 0.6330, Training Accuracy: 83.30%\n",
            "Epoch [69/100], Loss: 0.6317, Training Accuracy: 83.25%\n",
            "Epoch [70/100], Loss: 0.6281, Training Accuracy: 83.36%\n",
            "Epoch [71/100], Loss: 0.6267, Training Accuracy: 83.39%\n",
            "Epoch [72/100], Loss: 0.6263, Training Accuracy: 83.40%\n",
            "Epoch [73/100], Loss: 0.6239, Training Accuracy: 83.38%\n",
            "Epoch [74/100], Loss: 0.6249, Training Accuracy: 83.38%\n",
            "Epoch [75/100], Loss: 0.7642, Training Accuracy: 80.16%\n",
            "Epoch [76/100], Loss: 0.6227, Training Accuracy: 83.45%\n",
            "Epoch [77/100], Loss: 0.6217, Training Accuracy: 83.43%\n",
            "Epoch [78/100], Loss: 0.6214, Training Accuracy: 83.35%\n",
            "Epoch [79/100], Loss: 0.6209, Training Accuracy: 83.34%\n",
            "Epoch [80/100], Loss: 0.6199, Training Accuracy: 83.38%\n",
            "Epoch [81/100], Loss: 0.6212, Training Accuracy: 83.39%\n",
            "Epoch [82/100], Loss: 0.6204, Training Accuracy: 83.33%\n",
            "Epoch [83/100], Loss: 0.6206, Training Accuracy: 83.36%\n",
            "Epoch [84/100], Loss: 0.6207, Training Accuracy: 83.42%\n",
            "Epoch [85/100], Loss: 0.6188, Training Accuracy: 83.43%\n",
            "Epoch [86/100], Loss: 0.6190, Training Accuracy: 83.35%\n",
            "Epoch [87/100], Loss: 0.6180, Training Accuracy: 83.38%\n",
            "Epoch [88/100], Loss: 0.6184, Training Accuracy: 83.39%\n",
            "Epoch [89/100], Loss: 0.6207, Training Accuracy: 83.39%\n",
            "Epoch [90/100], Loss: 0.6186, Training Accuracy: 83.34%\n",
            "Epoch [91/100], Loss: 0.6885, Training Accuracy: 81.57%\n",
            "Epoch [92/100], Loss: 0.6158, Training Accuracy: 83.40%\n",
            "Epoch [93/100], Loss: 0.6154, Training Accuracy: 83.41%\n",
            "Epoch [94/100], Loss: 0.6154, Training Accuracy: 83.35%\n",
            "Epoch [95/100], Loss: 0.6163, Training Accuracy: 83.32%\n",
            "Epoch [96/100], Loss: 0.6155, Training Accuracy: 83.41%\n",
            "Epoch [97/100], Loss: 0.6149, Training Accuracy: 83.37%\n",
            "Epoch [98/100], Loss: 0.6159, Training Accuracy: 83.41%\n",
            "Epoch [99/100], Loss: 0.6158, Training Accuracy: 83.38%\n",
            "Epoch [100/100], Loss: 0.6150, Training Accuracy: 83.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-55b45ef5428f>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data = torch.load(data_file)  # Shape: (16655, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 44.10%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Preprocessing\n",
        "class DriverActivityDataset(Dataset):\n",
        "    def __init__(self, data_file, annotation_file):\n",
        "        # Load the .pt file (ensure the file is a tensor, not a list)\n",
        "        self.data = torch.load(data_file)  # Shape: (19071, 34)\n",
        "\n",
        "        # Ensure the data is a tensor and convert to np.argmax\n",
        "        self.data = torch.tensor(self.data)  # Convert list to tensor if it's not already\n",
        "        self.data = torch.argmax(self.data, dim=1)  # Shape: (19071,) - now contains np.argmax labels\n",
        "\n",
        "        # Load the CSV file with annotations\n",
        "        self.annotations = pd.read_csv(annotation_file)\n",
        "        self.annotations = self.annotations.iloc[:, 1].values  # Assuming annotations are in the second column\n",
        "\n",
        "        # Remove dummy -1 annotations\n",
        "        self.valid_data = []\n",
        "        self.valid_annotations = []\n",
        "        for i in range(len(self.annotations)):\n",
        "            if self.annotations[i] != -1:  # Only keep valid annotations\n",
        "                self.valid_data.append(self.data[i])\n",
        "                self.valid_annotations.append(self.annotations[i])\n",
        "\n",
        "        self.valid_data = torch.tensor(self.valid_data)\n",
        "        self.valid_annotations = torch.tensor(self.valid_annotations)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.valid_data[idx], self.valid_annotations[idx]\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, data_file, annotation_file):\n",
        "        # Load the .pt file (ensure the file is a tensor, not a list)\n",
        "        self.data = torch.load(data_file)  # Shape: (16655, 34)\n",
        "\n",
        "        # Convert the data into the appropriate format (same as the train dataset)\n",
        "        self.data = torch.tensor(self.data)\n",
        "        self.data = torch.argmax(self.data, dim=1)  # Shape: (16655,) - now contains np.argmax labels\n",
        "\n",
        "        # Load the CSV file with annotations\n",
        "        self.annotations = pd.read_csv(annotation_file)\n",
        "        self.annotations = self.annotations.iloc[:, 1].values  # Assuming annotations are in the second column\n",
        "\n",
        "        # Remove dummy -1 annotations\n",
        "        self.valid_data = []\n",
        "        self.valid_annotations = []\n",
        "        for i in range(len(self.annotations)):\n",
        "            if self.annotations[i] != -1:  # Only keep valid annotations\n",
        "                self.valid_data.append(self.data[i])\n",
        "                self.valid_annotations.append(self.annotations[i])\n",
        "\n",
        "        self.valid_data = torch.tensor(self.valid_data)\n",
        "        self.valid_annotations = torch.tensor(self.valid_annotations)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.valid_data[idx], self.valid_annotations[idx]\n",
        "\n",
        "\n",
        "# Model Architecture (LSTM)\n",
        "class TemporalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TemporalModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=128, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(128, 34)  # 34 actions, each action corresponds to a class\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure x has shape (batch_size, sequence_length, 1)\n",
        "        x = x.unsqueeze(-1).float()  # Add a dummy dimension for input size (batch_size, sequence_length, 1)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        lstm_out, _ = self.lstm(x)  # lstm_out will have shape (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        # Check if lstm_out is 3D, it should be\n",
        "        #print(f\"LSTM output shape: {lstm_out.shape}\")  # Debugging line\n",
        "\n",
        "        # Only use the output of the last frame (time step)\n",
        "        last_out = lstm_out[:, -1, :]  # Taking the last timestep output\n",
        "\n",
        "        # Feed last frame's LSTM output to the fully connected layer\n",
        "        output = self.fc(last_out)\n",
        "        return output\n",
        "\n",
        "# Load data\n",
        "train_file = '/content/run1b_2018-05-29-14-02-47.kinect_color.pt'  # .pt file containing class probability arrays\n",
        "train_annotations_file = '/content/new_data.csv'  # CSV file containing frame annotations\n",
        "\n",
        "train_dataset = DriverActivityDataset(train_file, train_annotations_file)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "def train_model(model, train_loader, val_loader=None, num_epochs=100, learning_rate=0.001, patience=10):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    epochs_without_improvement = 0  # Early stopping counter\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (frames, annotations) in enumerate(train_loader):\n",
        "            frames = frames.unsqueeze(-1).float()  # Reshape frames (batch_size, sequence_length, 1)\n",
        "            annotations = annotations.long()  # Ensure annotations are in long format\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(frames)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, annotations)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += annotations.size(0)\n",
        "            correct_train += (predicted == annotations).sum().item()\n",
        "\n",
        "        # Compute average loss and accuracy for the epoch\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Print statistics every epoch\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "        # Update learning rate with scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.7, verbose=True)\n",
        "\n",
        "        # Early stopping: Check for improvement in accuracy on validation set\n",
        "        if val_loader is not None:\n",
        "            val_accuracy = evaluate_model(model, val_loader)\n",
        "            print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                epochs_without_improvement = 0\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"Early stopping: No improvement for {patience} epochs.\")\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate and train the model\n",
        "model = TemporalModel()\n",
        "train_model(model, train_loader)\n",
        "\n",
        "\n",
        "def test_model(model, test_file, test_annotations_file):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_dataset = TestDataset(test_file, test_annotations_file)  # Use the new TestDataset class\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No gradients needed during inference\n",
        "        for frames, annotations in test_loader:\n",
        "            frames = frames.unsqueeze(-1).float()  # Reshape the frames as needed\n",
        "            annotations = annotations.long()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(frames)\n",
        "\n",
        "            # Get predictions (choose the class with the highest probability)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            total_samples += annotations.size(0)\n",
        "            total_correct += (predicted == annotations).sum().item()\n",
        "\n",
        "    accuracy = total_correct / total_samples * 100\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# Assuming test file is in .pt and .csv format\n",
        "test_file = '/content/run1_2018-05-22-15-10-41.kinect_color.pt'\n",
        "test_annotations_file = '/content/newtest.csv'\n",
        "test_model(model, test_file, test_annotations_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "class DriverActivityModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(DriverActivityModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout layer to prevent overfitting\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)  # Apply dropout after the activation\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)  # Softmax to get probability distribution over classes\n",
        "\n",
        "# Load the data\n",
        "data_file = '/content/run1b_2018-05-29-14-02-47.kinect_color.pt'\n",
        "train_data = torch.load(data_file)  # Shape: (19071, 34) for training data\n",
        "test_data_file = '/content/run1_2018-05-22-15-10-41.kinect_color.pt'\n",
        "test_data = torch.load(test_data_file)  # Shape: (16655, 34) for test data\n",
        "\n",
        "# Convert to tensors if necessary (if they are not already tensors)\n",
        "train_data = torch.tensor(train_data)  # Ensure it's a tensor\n",
        "test_data = torch.tensor(test_data)  # Ensure it's a tensor\n",
        "\n",
        "# Prepare input and output tensors\n",
        "X_train = train_data  # Input features (19071 x 34)\n",
        "y_train = torch.argmax(X_train, dim=1)  # Convert class probabilities to class labels\n",
        "X_test = test_data  # Test features (16655 x 34)\n",
        "y_test = torch.argmax(X_test, dim=1)  # Convert test data class probabilities\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = X_train.shape[1]  # 34 class probabilities\n",
        "hidden_size = 128  # You can adjust this\n",
        "output_size = 34  # Number of possible driver actions\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 100\n",
        "early_stopping_patience = 10  # Patience for early stopping\n",
        "best_test_accuracy = 0.0\n",
        "epochs_since_improvement = 0\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DriverActivityModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()  # Using cross entropy for classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)  # L2 regularization (weight_decay)\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # Training phase\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.float())  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = (correct_train / total_train) * 100\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.float())\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = (correct_test / total_test) * 100\n",
        "\n",
        "    # Print training and validation results\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, '\n",
        "          f'Training Accuracy: {train_accuracy:.2f}%, '\n",
        "          f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    # Check if early stopping condition is met\n",
        "    if test_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = test_accuracy\n",
        "        epochs_since_improvement = 0  # Reset the patience counter\n",
        "    else:\n",
        "        epochs_since_improvement += 1\n",
        "\n",
        "    if epochs_since_improvement >= early_stopping_patience:\n",
        "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
        "        break\n",
        "\n",
        "# Final test accuracy\n",
        "print(f'Final Test Accuracy: {best_test_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhGCoYecgT6L",
        "outputId": "8a0386b8-82b6-4d0d-f1e7-e6e243d0ae58"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-0f81425410d5>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_data = torch.load(data_file)  # Shape: (19071, 34) for training data\n",
            "<ipython-input-19-0f81425410d5>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data = torch.load(test_data_file)  # Shape: (16655, 34) for test data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 3.2043, Training Accuracy: 47.31%, Test Accuracy: 75.35%\n",
            "Epoch [2/100], Loss: 2.9332, Training Accuracy: 66.30%, Test Accuracy: 81.00%\n",
            "Epoch [3/100], Loss: 2.9141, Training Accuracy: 67.07%, Test Accuracy: 81.00%\n",
            "Epoch [4/100], Loss: 2.9105, Training Accuracy: 67.25%, Test Accuracy: 81.75%\n",
            "Epoch [5/100], Loss: 2.8926, Training Accuracy: 69.55%, Test Accuracy: 82.03%\n",
            "Epoch [6/100], Loss: 2.8878, Training Accuracy: 69.77%, Test Accuracy: 82.04%\n",
            "Epoch [7/100], Loss: 2.8877, Training Accuracy: 69.67%, Test Accuracy: 82.06%\n",
            "Epoch [8/100], Loss: 2.8869, Training Accuracy: 69.83%, Test Accuracy: 82.06%\n",
            "Epoch [9/100], Loss: 2.8867, Training Accuracy: 69.84%, Test Accuracy: 82.04%\n",
            "Epoch [10/100], Loss: 2.8866, Training Accuracy: 69.87%, Test Accuracy: 82.06%\n",
            "Epoch [11/100], Loss: 2.8863, Training Accuracy: 69.88%, Test Accuracy: 82.00%\n",
            "Epoch [12/100], Loss: 2.8861, Training Accuracy: 69.81%, Test Accuracy: 82.05%\n",
            "Epoch [13/100], Loss: 2.8858, Training Accuracy: 69.90%, Test Accuracy: 82.09%\n",
            "Epoch [14/100], Loss: 2.8858, Training Accuracy: 69.88%, Test Accuracy: 82.05%\n",
            "Epoch [15/100], Loss: 2.8858, Training Accuracy: 69.90%, Test Accuracy: 82.08%\n",
            "Epoch [16/100], Loss: 2.8859, Training Accuracy: 69.88%, Test Accuracy: 82.03%\n",
            "Epoch [17/100], Loss: 2.8858, Training Accuracy: 69.85%, Test Accuracy: 82.05%\n",
            "Epoch [18/100], Loss: 2.8865, Training Accuracy: 69.89%, Test Accuracy: 82.08%\n",
            "Epoch [19/100], Loss: 2.8861, Training Accuracy: 69.86%, Test Accuracy: 82.13%\n",
            "Epoch [20/100], Loss: 2.8860, Training Accuracy: 69.83%, Test Accuracy: 82.02%\n",
            "Epoch [21/100], Loss: 2.8857, Training Accuracy: 69.88%, Test Accuracy: 82.07%\n",
            "Epoch [22/100], Loss: 2.8858, Training Accuracy: 69.90%, Test Accuracy: 82.06%\n",
            "Epoch [23/100], Loss: 2.8858, Training Accuracy: 69.90%, Test Accuracy: 82.06%\n",
            "Epoch [24/100], Loss: 2.8858, Training Accuracy: 69.87%, Test Accuracy: 82.04%\n",
            "Epoch [25/100], Loss: 2.8856, Training Accuracy: 69.94%, Test Accuracy: 82.00%\n",
            "Epoch [26/100], Loss: 2.8860, Training Accuracy: 69.87%, Test Accuracy: 82.00%\n",
            "Epoch [27/100], Loss: 2.8859, Training Accuracy: 69.87%, Test Accuracy: 82.09%\n",
            "Epoch [28/100], Loss: 2.8860, Training Accuracy: 69.93%, Test Accuracy: 82.17%\n",
            "Epoch [29/100], Loss: 2.8857, Training Accuracy: 69.93%, Test Accuracy: 82.18%\n",
            "Epoch [30/100], Loss: 2.8854, Training Accuracy: 69.98%, Test Accuracy: 81.97%\n",
            "Epoch [31/100], Loss: 2.8858, Training Accuracy: 69.89%, Test Accuracy: 82.11%\n",
            "Epoch [32/100], Loss: 2.8853, Training Accuracy: 69.93%, Test Accuracy: 81.96%\n",
            "Epoch [33/100], Loss: 2.8857, Training Accuracy: 69.88%, Test Accuracy: 82.07%\n",
            "Epoch [34/100], Loss: 2.8860, Training Accuracy: 69.87%, Test Accuracy: 82.02%\n",
            "Epoch [35/100], Loss: 2.8859, Training Accuracy: 69.87%, Test Accuracy: 82.00%\n",
            "Epoch [36/100], Loss: 2.8854, Training Accuracy: 69.89%, Test Accuracy: 82.07%\n",
            "Epoch [37/100], Loss: 2.8860, Training Accuracy: 69.90%, Test Accuracy: 82.01%\n",
            "Epoch [38/100], Loss: 2.8858, Training Accuracy: 69.92%, Test Accuracy: 82.06%\n",
            "Epoch [39/100], Loss: 2.8863, Training Accuracy: 69.82%, Test Accuracy: 82.04%\n",
            "Early stopping triggered after 39 epochs.\n",
            "Final Test Accuracy: 82.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Load the data\n",
        "data_file = '/content/run1b_2018-05-29-14-02-47.kinect_color.pt'\n",
        "data = torch.load(data_file)  # Shape: (19071, 34) for training data\n",
        "test_data_file = '/content/run1_2018-05-22-15-10-41.kinect_color.pt'\n",
        "test_data = torch.load(test_data_file)  # Shape: (16655, 34) for test data\n",
        "\n",
        "# Convert loaded data (list) to tensors\n",
        "X_train = torch.tensor(data).float()  # Ensure X_train is a tensor\n",
        "y_train = torch.argmax(X_train, dim=1)  # Convert class probabilities to class labels\n",
        "X_test = torch.tensor(test_data).float()  # Ensure X_test is a tensor\n",
        "y_test = torch.argmax(X_test, dim=1)  # Convert test data class probabilities to labels\n",
        "\n",
        "# Create DataLoader for batch processing\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model architecture with a combination of TCN and RNN layers\n",
        "class TemporalDriverActivityModel(nn.Module):\n",
        "    def __init__(self, input_size=34, hidden_size=128, output_size=34, dropout_prob=0.5):\n",
        "        super(TemporalDriverActivityModel, self).__init__()\n",
        "\n",
        "        # Temporal Convolutional Network (TCN) block\n",
        "        self.tcn = nn.Conv1d(input_size, hidden_size, kernel_size=3, padding=1)\n",
        "\n",
        "        # Recurrent layer (LSTM)\n",
        "        self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Fully connected layer for output\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input for TCN: (batch_size, channels, sequence_length)\n",
        "        # Assuming sequence_length = 1 since your data doesn't seem to have a temporal sequence\n",
        "        x = x.unsqueeze(2)  # Add a singleton dimension to make it (batch_size, input_size, 1)\n",
        "\n",
        "        # Apply Temporal Convolution (TCN)\n",
        "        x = torch.relu(self.tcn(x))  # Shape becomes (batch_size, hidden_size, 1)\n",
        "\n",
        "        # Apply LSTM for temporal modeling\n",
        "        x, _ = self.rnn(x.transpose(1, 2))  # LSTM expects (batch_size, seq_len, input_size)\n",
        "\n",
        "        # Take the last output from LSTM\n",
        "        x = x[:, -1, :]\n",
        "\n",
        "        # Apply fully connected layer\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TemporalDriverActivityModel(input_size=34, hidden_size=128, output_size=34, dropout_prob=0.5)\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model with early stopping\n",
        "num_epochs = 100\n",
        "best_test_acc = 0\n",
        "early_stop_counter = 0\n",
        "early_stop_patience = 10  # Number of epochs without improvement to stop training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = (correct_train / total_train) * 100\n",
        "\n",
        "    # Evaluate on test data\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_accuracy = (correct_test / total_test) * 100\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
        "          f\"Training Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # Early stopping mechanism\n",
        "    if test_accuracy > best_test_acc:\n",
        "        best_test_acc = test_accuracy\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "\n",
        "    if early_stop_counter >= early_stop_patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(f\"Final Test Accuracy: {best_test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzFOurvQB4HF",
        "outputId": "18489693-b58e-4d7b-cc7e-cd196412c39a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-305870e2ea44>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(data_file)  # Shape: (19071, 34) for training data\n",
            "<ipython-input-22-305870e2ea44>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data = torch.load(test_data_file)  # Shape: (16655, 34) for test data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 2.1730, Training Accuracy: 38.92%, Test Accuracy: 93.65%\n",
            "Epoch [2/100], Loss: 1.5297, Training Accuracy: 49.90%, Test Accuracy: 96.48%\n",
            "Epoch [3/100], Loss: 1.4903, Training Accuracy: 50.11%, Test Accuracy: 97.21%\n",
            "Epoch [4/100], Loss: 1.4714, Training Accuracy: 50.50%, Test Accuracy: 97.31%\n",
            "Epoch [5/100], Loss: 1.4646, Training Accuracy: 50.48%, Test Accuracy: 97.50%\n",
            "Epoch [6/100], Loss: 1.4497, Training Accuracy: 50.82%, Test Accuracy: 97.73%\n",
            "Epoch [7/100], Loss: 1.4567, Training Accuracy: 50.46%, Test Accuracy: 97.58%\n",
            "Epoch [8/100], Loss: 1.4547, Training Accuracy: 50.45%, Test Accuracy: 97.70%\n",
            "Epoch [9/100], Loss: 1.4416, Training Accuracy: 50.92%, Test Accuracy: 97.84%\n",
            "Epoch [10/100], Loss: 1.4539, Training Accuracy: 50.38%, Test Accuracy: 97.63%\n",
            "Epoch [11/100], Loss: 1.4398, Training Accuracy: 50.86%, Test Accuracy: 97.74%\n",
            "Epoch [12/100], Loss: 1.4342, Training Accuracy: 50.99%, Test Accuracy: 97.58%\n",
            "Epoch [13/100], Loss: 1.4611, Training Accuracy: 50.12%, Test Accuracy: 97.77%\n",
            "Epoch [14/100], Loss: 1.4349, Training Accuracy: 50.95%, Test Accuracy: 97.78%\n",
            "Epoch [15/100], Loss: 1.4290, Training Accuracy: 51.27%, Test Accuracy: 97.67%\n",
            "Epoch [16/100], Loss: 1.4435, Training Accuracy: 50.66%, Test Accuracy: 97.65%\n",
            "Epoch [17/100], Loss: 1.4451, Training Accuracy: 50.64%, Test Accuracy: 97.78%\n",
            "Epoch [18/100], Loss: 1.4278, Training Accuracy: 51.10%, Test Accuracy: 97.79%\n",
            "Epoch [19/100], Loss: 1.4303, Training Accuracy: 51.08%, Test Accuracy: 97.95%\n",
            "Epoch [20/100], Loss: 1.4493, Training Accuracy: 50.39%, Test Accuracy: 97.75%\n",
            "Epoch [21/100], Loss: 1.4370, Training Accuracy: 50.84%, Test Accuracy: 97.75%\n",
            "Epoch [22/100], Loss: 1.4549, Training Accuracy: 50.12%, Test Accuracy: 97.87%\n",
            "Epoch [23/100], Loss: 1.4240, Training Accuracy: 51.19%, Test Accuracy: 97.97%\n",
            "Epoch [24/100], Loss: 1.4072, Training Accuracy: 51.63%, Test Accuracy: 97.87%\n",
            "Epoch [25/100], Loss: 1.4330, Training Accuracy: 50.76%, Test Accuracy: 97.72%\n",
            "Epoch [26/100], Loss: 1.4352, Training Accuracy: 50.81%, Test Accuracy: 97.82%\n",
            "Epoch [27/100], Loss: 1.4364, Training Accuracy: 50.68%, Test Accuracy: 97.77%\n",
            "Epoch [28/100], Loss: 1.4368, Training Accuracy: 50.68%, Test Accuracy: 97.99%\n",
            "Epoch [29/100], Loss: 1.4172, Training Accuracy: 51.42%, Test Accuracy: 98.08%\n",
            "Epoch [30/100], Loss: 1.4481, Training Accuracy: 50.32%, Test Accuracy: 97.95%\n",
            "Epoch [31/100], Loss: 1.4392, Training Accuracy: 50.71%, Test Accuracy: 98.19%\n",
            "Epoch [32/100], Loss: 1.4347, Training Accuracy: 50.81%, Test Accuracy: 98.00%\n",
            "Epoch [33/100], Loss: 1.4352, Training Accuracy: 50.80%, Test Accuracy: 97.70%\n",
            "Epoch [34/100], Loss: 1.4324, Training Accuracy: 50.97%, Test Accuracy: 98.04%\n",
            "Epoch [35/100], Loss: 1.4339, Training Accuracy: 50.83%, Test Accuracy: 98.00%\n",
            "Epoch [36/100], Loss: 1.4339, Training Accuracy: 50.93%, Test Accuracy: 97.98%\n",
            "Epoch [37/100], Loss: 1.4191, Training Accuracy: 51.27%, Test Accuracy: 98.00%\n",
            "Epoch [38/100], Loss: 1.4328, Training Accuracy: 50.84%, Test Accuracy: 97.82%\n",
            "Epoch [39/100], Loss: 1.4267, Training Accuracy: 51.03%, Test Accuracy: 98.01%\n",
            "Epoch [40/100], Loss: 1.4163, Training Accuracy: 51.54%, Test Accuracy: 98.03%\n",
            "Epoch [41/100], Loss: 1.4427, Training Accuracy: 50.54%, Test Accuracy: 97.95%\n",
            "Early stopping triggered.\n",
            "Final Test Accuracy: 98.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pTm5yJe7EBq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}